# LangGraph AI Implementation Guide for Engineers
## Sprint 004: Resume Analysis Multi-Agent System

**Document Version**: 1.0  
**Created**: September 4, 2025  
**Target Audience**: AI/ML Engineers, Backend Developers  
**Prerequisites**: Python 3.11+, FastAPI experience, basic LLM knowledge

---

## ðŸŽ¯ Overview

This guide provides comprehensive instructions for implementing the LangGraph-based AI orchestration system for resume analysis. You'll learn LangGraph fundamentals, our specific architecture patterns, and step-by-step implementation details.

## ðŸ“š Essential LangGraph Resources

### **Official Documentation (Read First)**
- **LangGraph Core Concepts**: https://langchain-ai.github.io/langgraph/concepts/
- **Multi-Agent Workflows**: https://langchain-ai.github.io/langgraph/concepts/multi_agent/
- **Workflow Tutorials**: https://langchain-ai.github.io/langgraph/tutorials/workflows/
- **StateGraph API Reference**: https://langchain-ai.github.io/langgraph/reference/graphs/

### **Key Learning Resources**
```bash
# Essential reading order:
1. LangGraph Multi-Agent Systems - Overview
2. Workflows & Agents Tutorial
3. State Management Concepts
4. Conditional Routing Patterns
5. Error Handling Best Practices
```

## ðŸ—ï¸ LangGraph Fundamentals

### **Core Concepts You Must Understand**

#### **1. StateGraph Workflow**
LangGraph uses a graph-based approach where:
- **Nodes** = Functions that process and update state
- **Edges** = Connections between nodes (normal or conditional)
- **State** = Shared data structure that flows through the workflow

```python
from langgraph.graph import StateGraph, START, END

# Basic pattern
workflow = StateGraph(StateType)
workflow.add_node("node_name", node_function)
workflow.add_edge(START, "node_name")
workflow.add_edge("node_name", END)
app = workflow.compile()
```

#### **2. State Management with TypedDict**
State is the heart of LangGraph workflows:

```python
from typing import TypedDict, List, Optional

class MyState(TypedDict):
    input_data: str
    processing_stage: str
    results: Optional[dict]
    errors: List[str]
    retry_count: int

# State flows through all nodes
def my_node(state: MyState) -> dict:
    # Process state and return updates
    return {"processing_stage": "completed", "results": {...}}
```

#### **3. Conditional Routing**
Dynamic workflow control based on state:

```python
def route_logic(state: MyState) -> str:
    if state.get("errors"):
        return "error_handler"
    return "next_step"

workflow.add_conditional_edges(
    "current_node",
    route_logic,
    {
        "next_step": "success_node",
        "error_handler": "error_node"
    }
)
```

## ðŸŽ¯ Our Resume Analysis Architecture

### **High-Level Workflow Design**

```python
# Our specific workflow pattern
START â†’ preprocess â†’ structure_agent â†’ structure_validation
                          â†“
  appeal_validation â† appeal_agent â† (conditional routing)
          â†“
  aggregate_results â†’ END
          
# Error handling at every step
Any Node â†’ error_handler â†’ END (with partial results)
```

### **State Schema Design**
Our `AnalysisState` schema is designed for resume analysis workflow:

```python
from typing import TypedDict, Optional, List, Literal, Dict, Any

class AnalysisState(TypedDict):
    # Input data
    resume_text: str                    # Raw resume text
    industry: str                       # Target industry
    analysis_id: str                    # Unique identifier
    user_id: str                        # User context
    
    # Workflow control
    current_stage: Literal[
        "preprocessing", "structure_analysis", "structure_validation",
        "appeal_analysis", "appeal_validation", "aggregation", 
        "complete", "error"
    ]
    
    # Agent results (structured data)
    structure_analysis: Optional[StructureAnalysisResult]
    structure_confidence: Optional[float]
    appeal_analysis: Optional[AppealAnalysisResult] 
    appeal_confidence: Optional[float]
    
    # Final output
    final_result: Optional[CompleteAnalysisResult]
    overall_score: Optional[float]
    
    # Error handling
    has_errors: bool
    error_messages: List[str]
    structure_errors: List[str]
    appeal_errors: List[str]
    retry_count: int
    max_retries: int
    
    # Metadata
    started_at: Optional[str]
    completed_at: Optional[str]
    processing_time_seconds: Optional[float]
```

## ðŸ› ï¸ Step-by-Step Implementation Guide

### **Step 1: Environment Setup**

```bash
# Install required dependencies
pip install langgraph>=0.2.0
pip install langchain>=0.2.0  
pip install langchain-openai>=0.1.0
pip install pydantic>=2.0.0
pip install tiktoken>=0.7.0

# For development
pip install pytest-asyncio>=0.23.0
```

### **Step 2: Project Structure Setup**

Create the AI module structure in your FastAPI backend:

```bash
backend/app/ai/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ orchestrator.py           # Main LangGraph orchestrator
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py         # Abstract base class
â”‚   â”œâ”€â”€ structure_agent.py    # Resume structure analysis
â”‚   â””â”€â”€ appeal_agent.py       # Industry-specific analysis
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ analysis_request.py   # State schemas
â”‚   â”œâ”€â”€ analysis_result.py    # Result models
â”‚   â””â”€â”€ agent_output.py       # Agent-specific models
â”œâ”€â”€ integrations/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_llm.py          # Abstract LLM interface
â”‚   â”œâ”€â”€ openai_client.py     # OpenAI integration
â”‚   â””â”€â”€ claude_client.py     # Claude integration (future)
â””â”€â”€ prompts/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ prompt_manager.py     # Dynamic prompt loading
    â””â”€â”€ templates/            # Prompt files
        â”œâ”€â”€ structure/
        â””â”€â”€ appeal/
```

### **Step 3: Core Models Implementation**

#### **State Models** (`/app/ai/models/analysis_request.py`)

```python
from typing import TypedDict, Optional, List, Literal, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime

class AnalysisState(TypedDict):
    # [Include the complete state schema from above]
    pass

class StructureAnalysisResult(BaseModel):
    """Structured output from Structure Agent"""
    
    # Core scores (0-100)
    format_score: float = Field(ge=0, le=100, description="Resume formatting quality")
    section_organization_score: float = Field(ge=0, le=100, description="Section organization") 
    professional_tone_score: float = Field(ge=0, le=100, description="Professional tone assessment")
    completeness_score: float = Field(ge=0, le=100, description="Information completeness")
    
    # Detailed feedback
    formatting_issues: List[str] = Field(description="Specific formatting problems identified")
    missing_sections: List[str] = Field(description="Expected resume sections not found")
    tone_problems: List[str] = Field(description="Unprofessional language instances")
    completeness_gaps: List[str] = Field(description="Missing critical information")
    
    # Positive feedback
    strengths: List[str] = Field(description="Well-executed structural elements")
    recommendations: List[str] = Field(description="Specific improvement suggestions")
    
    # Metadata
    confidence_score: float = Field(ge=0, le=1, description="Analysis confidence")
    processing_time_ms: int = Field(description="Processing time in milliseconds")
    model_used: str = Field(description="LLM model used for analysis")
    prompt_version: str = Field(description="Prompt template version")
    
    # Analytics
    total_sections_found: int = Field(description="Number of resume sections identified")
    word_count: int = Field(description="Total word count")
    estimated_reading_time_minutes: int = Field(description="Estimated reading time")

class AppealAnalysisResult(BaseModel):
    """Structured output from Appeal Agent"""
    
    # Core industry scores (0-100)
    achievement_relevance_score: float = Field(ge=0, le=100, description="Industry-relevant achievements")
    skills_alignment_score: float = Field(ge=0, le=100, description="Skills match for target industry") 
    experience_fit_score: float = Field(ge=0, le=100, description="Experience relevance to industry")
    competitive_positioning_score: float = Field(ge=0, le=100, description="Market competitiveness")
    
    # Industry-specific analysis
    relevant_achievements: List[str] = Field(description="Standout achievements for this industry")
    missing_skills: List[str] = Field(description="Critical skills gaps for industry")
    transferable_experience: List[str] = Field(description="Cross-industry experience value")
    industry_keywords: List[str] = Field(description="Industry-specific terms found")
    
    # Competitive analysis
    market_tier: Literal["entry", "mid", "senior", "executive"] = Field(description="Market level assessment")
    competitive_advantages: List[str] = Field(description="Unique selling points")
    improvement_areas: List[str] = Field(description="Enhancement opportunities")
    
    # Context integration
    structure_context_used: bool = Field(description="Whether structure analysis informed this analysis")
    
    # Metadata (inherited from base)
    confidence_score: float = Field(ge=0, le=1)
    processing_time_ms: int
    model_used: str
    prompt_version: str

class CompleteAnalysisResult(BaseModel):
    """Final aggregated result"""
    overall_score: float = Field(ge=0, le=100, description="Weighted overall score")
    structure_analysis: StructureAnalysisResult
    appeal_analysis: AppealAnalysisResult
    
    # Summary
    analysis_summary: str = Field(description="Executive summary of analysis")
    key_strengths: List[str] = Field(description="Top 3-5 resume strengths")
    priority_improvements: List[str] = Field(description="Top 3-5 improvement areas")
    
    # Context
    industry: str = Field(description="Target industry analyzed")
    analysis_id: str = Field(description="Unique analysis identifier")
    
    # Timestamps
    completed_at: str = Field(description="ISO timestamp of completion")
    processing_time_seconds: float = Field(description="Total processing time")
```

### **Step 4: Base Agent Implementation**

#### **Abstract Base Agent** (`/app/ai/agents/base_agent.py`)

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from app.ai.integrations.base_llm import BaseLLM
from app.ai.models.analysis_request import AnalysisState
from app.ai.prompts.prompt_manager import PromptManager
import time
import logging

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """Abstract base class for all resume analysis agents"""
    
    def __init__(self, llm_client: BaseLLM):
        self.llm = llm_client
        self.prompt_manager = PromptManager()
        
    @abstractmethod
    async def analyze(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Main analysis method that must be implemented by each agent.
        
        Args:
            state: Current workflow state containing resume text and context
            
        Returns:
            Dict containing state updates for the workflow
        """
        pass
    
    def _calculate_confidence(self, raw_output: str, expected_fields: List[str]) -> float:
        """Calculate confidence score based on output completeness and quality"""
        if not raw_output or len(raw_output) < 100:
            return 0.3
            
        # Check for expected fields/content
        found_fields = sum(1 for field in expected_fields if field.lower() in raw_output.lower())
        field_confidence = found_fields / len(expected_fields)
        
        # Check for reasonable length and structure
        length_score = min(len(raw_output) / 1000, 1.0)  # Normalize to 1000 chars
        
        # Combine scores
        confidence = (field_confidence * 0.7) + (length_score * 0.3)
        return min(confidence, 1.0)
    
    def _handle_agent_error(self, error: Exception, state: AnalysisState) -> Dict[str, Any]:
        """Standardized error handling for all agents"""
        error_msg = f"{self.__class__.__name__} error: {str(error)}"
        logger.error(error_msg, exc_info=True)
        
        return {
            "has_errors": True,
            "error_messages": [error_msg],
            "retry_count": state.get("retry_count", 0) + 1
        }
        
    def _get_processing_time_ms(self, start_time: float) -> int:
        """Calculate processing time in milliseconds"""
        return int((time.time() - start_time) * 1000)
```

### **Step 5: Structure Agent Implementation**

#### **Structure Agent** (`/app/ai/agents/structure_agent.py`)

```python
from typing import Dict, Any, List
from app.ai.agents.base_agent import BaseAgent
from app.ai.models.analysis_request import AnalysisState, StructureAnalysisResult
import time
import json
import re
import logging

logger = logging.getLogger(__name__)

class StructureAgent(BaseAgent):
    """Analyzes resume structure, formatting, and professional presentation"""
    
    async def analyze(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Perform comprehensive structure analysis of resume
        
        This is a LangGraph node function that:
        1. Loads structure analysis prompt
        2. Calls LLM for analysis
        3. Parses structured output
        4. Returns state updates for workflow
        """
        start_time = time.time()
        
        try:
            logger.info(f"Starting structure analysis for {state['analysis_id']}")
            
            # Load structure analysis prompt (industry-agnostic)
            prompt = self.prompt_manager.get_prompt(
                agent_type="structure",
                industry="general"  # Structure analysis doesn't depend on industry
            )
            
            # Prepare analysis context
            analysis_context = {
                "resume_text": state["resume_text"],
                "analysis_id": state["analysis_id"],
                "word_count": len(state["resume_text"].split()),
                "instructions": "Analyze the structural quality, formatting, and professional presentation of this resume."
            }
            
            # Execute LLM analysis
            formatted_prompt = prompt.format(**analysis_context)
            raw_result = await self.llm.ainvoke(formatted_prompt)
            
            # Parse structured output
            structured_result = self._parse_structure_output(
                raw_result.content, 
                state["resume_text"]
            )
            
            # Calculate confidence score
            confidence = self._calculate_structure_confidence(structured_result, raw_result.content)
            
            # Update processing metadata
            structured_result.processing_time_ms = self._get_processing_time_ms(start_time)
            structured_result.confidence_score = confidence
            
            logger.info(
                f"Structure analysis completed for {state['analysis_id']} "
                f"(confidence: {confidence:.2f}, time: {structured_result.processing_time_ms}ms)"
            )
            
            # Return state updates for LangGraph workflow
            return {
                "structure_analysis": structured_result,
                "structure_confidence": confidence,
                "current_stage": "structure_analysis",
                "structure_errors": []
            }
            
        except Exception as e:
            return self._handle_agent_error(e, state)
    
    def _parse_structure_output(self, raw_output: str, resume_text: str) -> StructureAnalysisResult:
        """Parse LLM output into structured StructureAnalysisResult"""
        try:
            # This is a simplified parser - in production, you might want to use
            # LLM structured output or more sophisticated parsing
            
            # Extract sections analysis
            sections_found = self._identify_resume_sections(resume_text)
            word_count = len(resume_text.split())
            
            # Parse scores from LLM output (assume JSON-like output)
            scores = self._extract_scores_from_output(raw_output)
            
            # Parse feedback lists
            feedback = self._extract_feedback_from_output(raw_output)
            
            return StructureAnalysisResult(
                format_score=scores.get("format_score", 75.0),
                section_organization_score=scores.get("section_organization_score", 75.0),
                professional_tone_score=scores.get("professional_tone_score", 75.0),
                completeness_score=scores.get("completeness_score", 75.0),
                
                formatting_issues=feedback.get("formatting_issues", []),
                missing_sections=feedback.get("missing_sections", []),
                tone_problems=feedback.get("tone_problems", []),
                completeness_gaps=feedback.get("completeness_gaps", []),
                
                strengths=feedback.get("strengths", []),
                recommendations=feedback.get("recommendations", []),
                
                total_sections_found=len(sections_found),
                word_count=word_count,
                estimated_reading_time_minutes=max(1, word_count // 200),
                
                model_used="gpt-4",
                prompt_version="structure_v1.0",
                confidence_score=0.0,  # Will be updated by calling function
                processing_time_ms=0   # Will be updated by calling function
            )
            
        except Exception as e:
            logger.error(f"Error parsing structure output: {str(e)}")
            # Return default result on parsing error
            return self._create_default_structure_result(resume_text)
    
    def _identify_resume_sections(self, resume_text: str) -> List[str]:
        """Identify major resume sections"""
        common_sections = [
            "experience", "education", "skills", "summary", "objective",
            "projects", "certifications", "awards", "publications"
        ]
        
        found_sections = []
        text_lower = resume_text.lower()
        
        for section in common_sections:
            if section in text_lower:
                found_sections.append(section.title())
                
        return found_sections
    
    def _extract_scores_from_output(self, raw_output: str) -> Dict[str, float]:
        """Extract numerical scores from LLM output"""
        scores = {}
        
        # Look for patterns like "format_score: 85" or "Format Score: 85/100"
        score_patterns = [
            r"format[_\s]*score[:\s]*(\d+)",
            r"section[_\s]*organization[_\s]*score[:\s]*(\d+)",
            r"professional[_\s]*tone[_\s]*score[:\s]*(\d+)",
            r"completeness[_\s]*score[:\s]*(\d+)"
        ]
        
        score_keys = ["format_score", "section_organization_score", 
                     "professional_tone_score", "completeness_score"]
        
        for pattern, key in zip(score_patterns, score_keys):
            match = re.search(pattern, raw_output, re.IGNORECASE)
            if match:
                scores[key] = float(match.group(1))
            else:
                scores[key] = 75.0  # Default score
                
        return scores
    
    def _extract_feedback_from_output(self, raw_output: str) -> Dict[str, List[str]]:
        """Extract feedback lists from LLM output"""
        feedback = {
            "formatting_issues": [],
            "missing_sections": [],
            "tone_problems": [],
            "completeness_gaps": [],
            "strengths": [],
            "recommendations": []
        }
        
        # This is simplified - in production, you'd want more sophisticated parsing
        # or use LLM structured output capabilities
        
        lines = raw_output.split('\n')
        current_category = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Detect category headers
            line_lower = line.lower()
            if "formatting issue" in line_lower or "format problem" in line_lower:
                current_category = "formatting_issues"
            elif "missing section" in line_lower:
                current_category = "missing_sections"
            elif "tone problem" in line_lower:
                current_category = "tone_problems"
            elif "completeness gap" in line_lower or "missing information" in line_lower:
                current_category = "completeness_gaps"
            elif "strength" in line_lower:
                current_category = "strengths"
            elif "recommendation" in line_lower:
                current_category = "recommendations"
            elif line.startswith('-') or line.startswith('*') or line.startswith('â€¢'):
                # This is a list item
                if current_category and current_category in feedback:
                    item = line.lstrip('-*â€¢ ').strip()
                    if item:
                        feedback[current_category].append(item)
        
        return feedback
    
    def _calculate_structure_confidence(self, result: StructureAnalysisResult, raw_output: str) -> float:
        """Calculate confidence specific to structure analysis"""
        # Check if we have reasonable scores
        scores = [
            result.format_score, result.section_organization_score,
            result.professional_tone_score, result.completeness_score
        ]
        
        # Confidence based on score reasonableness
        reasonable_scores = sum(1 for score in scores if 0 <= score <= 100)
        score_confidence = reasonable_scores / len(scores)
        
        # Confidence based on feedback completeness
        feedback_items = (
            len(result.strengths) + len(result.recommendations) +
            len(result.formatting_issues) + len(result.completeness_gaps)
        )
        feedback_confidence = min(feedback_items / 10, 1.0)  # Normalize to 10 items
        
        # Overall confidence
        overall_confidence = (score_confidence * 0.6) + (feedback_confidence * 0.4)
        
        return max(0.3, min(overall_confidence, 1.0))  # Clamp between 0.3 and 1.0
    
    def _create_default_structure_result(self, resume_text: str) -> StructureAnalysisResult:
        """Create a default result when parsing fails"""
        return StructureAnalysisResult(
            format_score=70.0,
            section_organization_score=70.0,
            professional_tone_score=70.0,
            completeness_score=70.0,
            
            formatting_issues=["Unable to analyze formatting due to parsing error"],
            missing_sections=[],
            tone_problems=[],
            completeness_gaps=[],
            
            strengths=["Resume structure could not be fully analyzed"],
            recommendations=["Please retry analysis or contact support"],
            
            total_sections_found=0,
            word_count=len(resume_text.split()),
            estimated_reading_time_minutes=max(1, len(resume_text.split()) // 200),
            
            confidence_score=0.3,
            processing_time_ms=0,
            model_used="gpt-4",
            prompt_version="structure_v1.0"
        )
```

### **Step 6: LangGraph Orchestrator Implementation**

#### **Main Orchestrator** (`/app/ai/orchestrator.py`)

```python
from langgraph.graph import StateGraph, START, END
from typing import Dict, Any, Literal
from app.ai.models.analysis_request import AnalysisState, CompleteAnalysisResult
from app.ai.agents.structure_agent import StructureAgent
from app.ai.agents.appeal_agent import AppealAgent
from app.ai.integrations.base_llm import BaseLLM
from app.core.datetime_utils import utc_now
import time
import logging

logger = logging.getLogger(__name__)

class ResumeAnalysisOrchestrator:
    """
    Main LangGraph orchestrator for multi-agent resume analysis
    
    This class implements the complete workflow:
    START â†’ preprocess â†’ structure_analysis â†’ structure_validation
    â†’ appeal_analysis â†’ appeal_validation â†’ aggregate_results â†’ END
    
    With error handling and retry logic at each step.
    """
    
    def __init__(self, llm_client: BaseLLM):
        self.llm_client = llm_client
        
        # Initialize agents
        self.structure_agent = StructureAgent(llm_client)
        self.appeal_agent = AppealAgent(llm_client)  # Will implement next
        
        # Build and compile workflow
        self.workflow = self._build_workflow()
        self.app = self.workflow.compile()
        
    def _build_workflow(self) -> StateGraph:
        """
        Build the complete LangGraph workflow for resume analysis
        
        This is the core of our AI orchestration system.
        Each node is a function that receives state and returns state updates.
        """
        workflow = StateGraph(AnalysisState)
        
        # Add all workflow nodes
        workflow.add_node("preprocess", self.preprocess_resume)
        workflow.add_node("structure_analysis", self.run_structure_agent)
        workflow.add_node("structure_validation", self.validate_structure_results)
        workflow.add_node("appeal_analysis", self.run_appeal_agent)
        workflow.add_node("appeal_validation", self.validate_appeal_results)
        workflow.add_node("aggregate_results", self.aggregate_final_results)
        workflow.add_node("handle_error", self.handle_analysis_error)
        
        # Define workflow flow with conditional routing
        workflow.add_edge(START, "preprocess")
        
        # Preprocessing â†’ Structure Analysis (with error handling)
        workflow.add_conditional_edges(
            "preprocess",
            self.route_after_preprocess,
            {
                "continue": "structure_analysis",
                "error": "handle_error"
            }
        )
        
        # Structure Analysis â†’ Validation (with retry logic)
        workflow.add_conditional_edges(
            "structure_analysis", 
            self.route_after_structure,
            {
                "validate": "structure_validation",
                "retry": "structure_analysis",
                "error": "handle_error"
            }
        )
        
        # Structure Validation â†’ Appeal Analysis (with retry)
        workflow.add_conditional_edges(
            "structure_validation",
            self.route_after_structure_validation, 
            {
                "continue": "appeal_analysis",
                "retry": "structure_analysis",
                "error": "handle_error"
            }
        )
        
        # Appeal Analysis â†’ Validation
        workflow.add_conditional_edges(
            "appeal_analysis",
            self.route_after_appeal,
            {
                "validate": "appeal_validation", 
                "retry": "appeal_analysis",
                "error": "handle_error"
            }
        )
        
        # Appeal Validation â†’ Results Aggregation
        workflow.add_conditional_edges(
            "appeal_validation",
            self.route_after_appeal_validation,
            {
                "continue": "aggregate_results",
                "retry": "appeal_analysis", 
                "error": "handle_error"
            }
        )
        
        # Terminal nodes
        workflow.add_edge("aggregate_results", END)
        workflow.add_edge("handle_error", END)
        
        return workflow
    
    # Node Implementation Functions
    # Each function receives state and returns state updates
    
    def preprocess_resume(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Preprocess and validate resume text before analysis
        
        LangGraph Node Function:
        - Validates input data
        - Cleans and normalizes resume text  
        - Sets up workflow metadata
        - Returns state updates for next node
        """
        logger.info(f"Starting preprocessing for analysis {state['analysis_id']}")
        
        resume_text = state["resume_text"].strip()
        
        # Input validation
        if not resume_text:
            return {
                "has_errors": True,
                "error_messages": ["No resume text provided for analysis"],
                "current_stage": "preprocessing"
            }
        
        if len(resume_text) < 100:
            return {
                "has_errors": True,
                "error_messages": ["Resume text too short for meaningful analysis (minimum 100 characters)"],
                "current_stage": "preprocessing"
            }
        
        if len(resume_text) > 50000:
            return {
                "has_errors": True,
                "error_messages": ["Resume text exceeds maximum length (50,000 characters)"],
                "current_stage": "preprocessing"
            }
        
        # Industry validation
        supported_industries = [
            "tech_consulting", "system_integrator", "finance_banking",
            "healthcare_pharma", "manufacturing", "general_business"
        ]
        
        if state["industry"] not in supported_industries:
            return {
                "has_errors": True,
                "error_messages": [f"Unsupported industry: {state['industry']}. Supported: {', '.join(supported_industries)}"],
                "current_stage": "preprocessing"
            }
        
        # Text cleaning and normalization
        processed_text = self._clean_resume_text(resume_text)
        
        # Set up workflow metadata
        return {
            "resume_text": processed_text,
            "current_stage": "preprocessing",
            "started_at": utc_now().isoformat(),
            "has_errors": False,
            "error_messages": [],
            "retry_count": 0,
            "max_retries": 2
        }
    
    async def run_structure_agent(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Execute structure analysis agent
        
        LangGraph Node Function:
        - Calls StructureAgent.analyze()
        - Handles agent-specific errors
        - Returns structured analysis results
        """
        logger.info(f"Running structure analysis for {state['analysis_id']}")
        
        try:
            result = await self.structure_agent.analyze(state)
            logger.info(f"Structure analysis completed for {state['analysis_id']}")
            return result
            
        except Exception as e:
            logger.error(f"Structure agent failed for {state['analysis_id']}: {str(e)}")
            return {
                "structure_errors": [f"Structure analysis failed: {str(e)}"],
                "has_errors": True,
                "current_stage": "structure_analysis"
            }
    
    def validate_structure_results(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Validate structure analysis results for quality and completeness
        
        LangGraph Node Function:
        - Checks analysis quality and confidence
        - Determines if retry is needed
        - Returns validation decision
        """
        structure_analysis = state.get("structure_analysis")
        confidence = state.get("structure_confidence", 0.0)
        
        logger.info(f"Validating structure results for {state['analysis_id']} (confidence: {confidence:.2f})")
        
        # Check if analysis was successful
        if not structure_analysis:
            return {
                "has_errors": True,
                "error_messages": ["Structure analysis produced no results"],
                "structure_errors": ["Analysis failed to complete"],
                "current_stage": "structure_validation"
            }
        
        # Confidence threshold check
        confidence_threshold = 0.6
        if confidence < confidence_threshold:
            retry_count = state.get("retry_count", 0)
            max_retries = state.get("max_retries", 2)
            
            if retry_count < max_retries:
                logger.warning(f"Structure analysis confidence too low ({confidence:.2f} < {confidence_threshold}), retrying...")
                return {
                    "structure_errors": [f"Low confidence score: {confidence:.2f}, retrying analysis"],
                    "retry_count": retry_count + 1,
                    "current_stage": "structure_validation"
                }
            else:
                logger.error(f"Structure analysis confidence still too low after {retry_count} retries")
                return {
                    "has_errors": True,
                    "error_messages": [f"Structure analysis confidence too low after {retry_count} retries"],
                    "current_stage": "structure_validation"
                }
        
        # Validate required fields exist
        required_scores = ['format_score', 'section_organization_score', 'professional_tone_score', 'completeness_score']
        missing_scores = [score for score in required_scores if not hasattr(structure_analysis, score)]
        
        if missing_scores:
            return {
                "structure_errors": [f"Missing required scores: {', '.join(missing_scores)}"],
                "has_errors": True,
                "current_stage": "structure_validation"
            }
        
        # Validation passed
        logger.info(f"Structure validation passed for {state['analysis_id']}")
        return {
            "current_stage": "structure_validated",
            "structure_errors": []
        }
    
    async def run_appeal_agent(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Execute appeal analysis agent with structure context
        
        LangGraph Node Function:
        - Calls AppealAgent.analyze() with structure results as context
        - Handles agent-specific errors
        - Returns industry-specific analysis results
        """
        logger.info(f"Running appeal analysis for {state['analysis_id']} (industry: {state['industry']})")
        
        try:
            result = await self.appeal_agent.analyze(state)
            logger.info(f"Appeal analysis completed for {state['analysis_id']}")
            return result
            
        except Exception as e:
            logger.error(f"Appeal agent failed for {state['analysis_id']}: {str(e)}")
            return {
                "appeal_errors": [f"Appeal analysis failed: {str(e)}"],
                "has_errors": True,
                "current_stage": "appeal_analysis"
            }
    
    def validate_appeal_results(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Validate appeal analysis results with industry-specific checks
        
        LangGraph Node Function:
        - Validates appeal analysis quality
        - Checks industry-specific requirements
        - Determines if retry is needed
        """
        appeal_analysis = state.get("appeal_analysis")
        confidence = state.get("appeal_confidence", 0.0)
        
        logger.info(f"Validating appeal results for {state['analysis_id']} (confidence: {confidence:.2f})")
        
        if not appeal_analysis:
            return {
                "has_errors": True,
                "error_messages": ["Appeal analysis produced no results"],
                "appeal_errors": ["Analysis failed to complete"],
                "current_stage": "appeal_validation"
            }
        
        # Industry-specific validation
        industry = state["industry"]
        if not self._validate_industry_analysis(appeal_analysis, industry):
            retry_count = state.get("retry_count", 0)
            max_retries = state.get("max_retries", 2)
            
            if retry_count < max_retries:
                logger.warning(f"Appeal analysis incomplete for {industry}, retrying...")
                return {
                    "appeal_errors": [f"Industry analysis incomplete for {industry}, retrying"],
                    "retry_count": retry_count + 1,
                    "current_stage": "appeal_validation"
                }
        
        # Confidence check
        confidence_threshold = 0.65
        if confidence < confidence_threshold:
            retry_count = state.get("retry_count", 0)
            max_retries = state.get("max_retries", 2)
            
            if retry_count < max_retries:
                return {
                    "appeal_errors": [f"Low confidence score: {confidence:.2f}, retrying"],
                    "retry_count": retry_count + 1,
                    "current_stage": "appeal_validation"
                }
        
        logger.info(f"Appeal validation passed for {state['analysis_id']}")
        return {
            "current_stage": "appeal_validated",
            "appeal_errors": []
        }
    
    def aggregate_final_results(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Combine structure and appeal analysis into comprehensive final result
        
        LangGraph Node Function:
        - Aggregates results from both agents
        - Calculates weighted overall score
        - Generates executive summary
        - Returns complete analysis result
        """
        logger.info(f"Aggregating final results for {state['analysis_id']}")
        
        structure = state["structure_analysis"]
        appeal = state["appeal_analysis"]
        
        # Calculate weighted overall score
        structure_weight = 0.35  # 35% weight for structure
        appeal_weight = 0.65     # 65% weight for industry appeal
        
        # Structure component (average of 4 core scores)
        structure_avg = (
            structure.format_score +
            structure.section_organization_score +
            structure.professional_tone_score +
            structure.completeness_score
        ) / 4
        
        # Appeal component (average of 4 core scores)  
        appeal_avg = (
            appeal.achievement_relevance_score +
            appeal.skills_alignment_score +
            appeal.experience_fit_score +
            appeal.competitive_positioning_score
        ) / 4
        
        # Weighted overall score
        overall_score = (structure_weight * structure_avg) + (appeal_weight * appeal_avg)
        
        # Generate summary and key insights
        analysis_summary = self._generate_executive_summary(structure, appeal, overall_score)
        key_strengths = self._extract_key_strengths(structure, appeal)
        priority_improvements = self._extract_priority_improvements(structure, appeal)
        
        # Create comprehensive final result
        final_result = CompleteAnalysisResult(
            overall_score=round(overall_score, 2),
            structure_analysis=structure,
            appeal_analysis=appeal,
            
            analysis_summary=analysis_summary,
            key_strengths=key_strengths,
            priority_improvements=priority_improvements,
            
            industry=state["industry"],
            analysis_id=state["analysis_id"],
            completed_at=utc_now().isoformat(),
            processing_time_seconds=self._calculate_total_processing_time(state)
        )
        
        logger.info(f"Analysis completed for {state['analysis_id']} with overall score: {overall_score:.2f}")
        
        return {
            "final_result": final_result,
            "overall_score": overall_score,
            "current_stage": "complete",
            "completed_at": utc_now().isoformat()
        }
    
    def handle_analysis_error(self, state: AnalysisState) -> Dict[str, Any]:
        """
        Handle analysis errors with graceful degradation
        
        LangGraph Node Function:
        - Processes workflow errors
        - Attempts to provide partial results
        - Returns error state for workflow termination
        """
        error_messages = state.get("error_messages", [])
        current_stage = state.get("current_stage", "unknown")
        
        logger.error(f"Analysis error in stage {current_stage} for {state['analysis_id']}: {error_messages}")
        
        # Try to provide partial results if possible
        partial_result = self._create_partial_result(state)
        
        return {
            "has_errors": True,
            "error_messages": error_messages,
            "current_stage": "error",
            "final_result": partial_result,
            "completed_at": utc_now().isoformat()
        }
    
    # Routing Functions for Conditional Edges
    
    def route_after_preprocess(self, state: AnalysisState) -> str:
        """Route after preprocessing based on validation results"""
        if state.get("has_errors"):
            return "error"
        return "continue"
    
    def route_after_structure(self, state: AnalysisState) -> str:
        """Route after structure analysis based on success/failure"""
        if state.get("structure_errors"):
            retry_count = state.get("retry_count", 0)
            max_retries = state.get("max_retries", 2)
            
            if retry_count < max_retries:
                return "retry"
            else:
                return "error"
        
        return "validate"
    
    def route_after_structure_validation(self, state: AnalysisState) -> str:
        """Route after structure validation"""
        if state.get("has_errors"):
            return "error"
        
        if state.get("structure_errors"):
            retry_count = state.get("retry_count", 0)
            max_retries = state.get("max_retries", 2)
            
            if retry_count < max_retries:
                return "retry"
            else:
                return "error"
        
        return "continue"
    
    def route_after_appeal(self, state: AnalysisState) -> str:
        """Route after appeal analysis"""
        if state.get("appeal_errors"):
            retry_count = state.get("retry_count", 0)
            max_retries = state.get("max_retries", 2)
            
            if retry_count < max_retries:
                return "retry"
            else:
                return "error"
        
        return "validate"
    
    def route_after_appeal_validation(self, state: AnalysisState) -> str:
        """Route after appeal validation"""
        if state.get("has_errors"):
            return "error"
            
        if state.get("appeal_errors"):
            retry_count = state.get("retry_count", 0) 
            max_retries = state.get("max_retries", 2)
            
            if retry_count < max_retries:
                return "retry"
            else:
                return "error"
        
        return "continue"
    
    # Helper Methods
    
    def _clean_resume_text(self, text: str) -> str:
        """Clean and normalize resume text for analysis"""
        # Remove excessive whitespace
        import re
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters that might interfere with LLM processing
        text = re.sub(r'[^\w\s\-\.\,\;\:\!\?\(\)\/\@\#\&\*\+\=\[\]\_\{\}\|\~\`\'\"]', '', text)
        
        return text.strip()
    
    def _validate_industry_analysis(self, appeal_analysis, industry: str) -> bool:
        """Validate that appeal analysis is complete for the specified industry"""
        # Check that we have reasonable industry-specific results
        required_fields = [
            appeal_analysis.relevant_achievements,
            appeal_analysis.missing_skills,
            appeal_analysis.competitive_advantages
        ]
        
        # All fields should have at least some content
        return all(len(field) > 0 for field in required_fields)
    
    def _generate_executive_summary(self, structure, appeal, overall_score: float) -> str:
        """Generate executive summary of the analysis"""
        score_tier = "excellent" if overall_score >= 85 else "strong" if overall_score >= 70 else "moderate" if overall_score >= 55 else "needs improvement"
        
        return (
            f"This resume demonstrates {score_tier} quality with an overall score of {overall_score:.1f}/100. "
            f"Structure analysis shows strengths in {', '.join(structure.strengths[:2]) if structure.strengths else 'basic formatting'}. "
            f"For the {appeal.industry if hasattr(appeal, 'industry') else 'target'} industry, "
            f"the candidate shows {appeal.market_tier}-level positioning with notable achievements in "
            f"{', '.join(appeal.relevant_achievements[:2]) if appeal.relevant_achievements else 'their experience'}."
        )
    
    def _extract_key_strengths(self, structure, appeal) -> List[str]:
        """Extract top strengths from both analyses"""
        strengths = []
        
        # Add top structure strengths
        strengths.extend(structure.strengths[:2])
        
        # Add top appeal strengths
        strengths.extend(appeal.competitive_advantages[:2])
        
        return strengths[:5]  # Return top 5
    
    def _extract_priority_improvements(self, structure, appeal) -> List[str]:
        """Extract priority improvement areas"""
        improvements = []
        
        # Add top structure recommendations
        improvements.extend(structure.recommendations[:2])
        
        # Add top appeal improvements
        improvements.extend(appeal.improvement_areas[:2])
        
        return improvements[:5]  # Return top 5
    
    def _calculate_total_processing_time(self, state: AnalysisState) -> float:
        """Calculate total processing time for the analysis"""
        if state.get("started_at"):
            from datetime import datetime
            started = datetime.fromisoformat(state["started_at"])
            now = utc_now()
            return (now - started).total_seconds()
        return 0.0
    
    def _create_partial_result(self, state: AnalysisState):
        """Create partial result when analysis fails"""
        # Implementation depends on what analysis completed
        # This is a fallback to provide some useful information even on failure
        return None
```

## ðŸ”§ Implementation Best Practices

### **LangGraph Development Patterns**

#### **1. State Design Principles**
```python
# âœ… Good: Comprehensive state with clear typing
class AnalysisState(TypedDict):
    # Input data (immutable during workflow)
    resume_text: str
    industry: str
    
    # Workflow state (updated by nodes)
    current_stage: Literal["preprocessing", "analysis", ...]
    
    # Results (structured data)
    structure_analysis: Optional[StructureAnalysisResult]
    
    # Error handling (always included)
    has_errors: bool
    error_messages: List[str]
    retry_count: int

# âŒ Bad: Minimal state that lacks error handling
class BadState(TypedDict):
    text: str
    result: Optional[dict]  # Untyped results
```

#### **2. Node Function Patterns**
```python
# âœ… Good: Node function with proper error handling
def my_node(state: AnalysisState) -> Dict[str, Any]:
    try:
        # Do work
        result = process_data(state["input_data"])
        
        # Return state updates
        return {
            "current_stage": "completed",
            "results": result,
            "errors": []
        }
    except Exception as e:
        # Always handle errors in nodes
        return {
            "has_errors": True,
            "error_messages": [str(e)],
            "current_stage": "error"
        }

# âŒ Bad: Node without error handling
def bad_node(state: AnalysisState) -> Dict[str, Any]:
    result = process_data(state["input_data"])  # Can crash workflow
    return {"results": result}
```

#### **3. Conditional Routing Patterns**
```python
# âœ… Good: Clear routing logic with error handling
def route_after_analysis(state: AnalysisState) -> str:
    if state.get("has_errors"):
        return "error_handler"
    
    if state.get("analysis_errors"):
        retry_count = state.get("retry_count", 0)
        if retry_count < MAX_RETRIES:
            return "retry"
        else:
            return "error_handler"
    
    return "next_step"

# âŒ Bad: Routing without error cases
def bad_routing(state: AnalysisState) -> str:
    if state.get("confidence") > 0.8:
        return "next_step"
    return "retry"  # What about errors?
```

## ðŸ§ª Testing Your Implementation

### **Unit Testing LangGraph Components**

```python
# /tests/unit/ai/test_orchestrator.py
import pytest
from unittest.mock import AsyncMock, MagicMock
from app.ai.orchestrator import ResumeAnalysisOrchestrator
from app.ai.models.analysis_request import AnalysisState

@pytest.fixture
def mock_llm():
    mock = AsyncMock()
    mock.ainvoke.return_value = MagicMock(content="Mock LLM response with scores and analysis")
    return mock

@pytest.fixture
def orchestrator(mock_llm):
    return ResumeAnalysisOrchestrator(mock_llm)

@pytest.fixture
def sample_state():
    return AnalysisState(
        resume_text="John Smith\nSoftware Engineer\n\nExperience:\n- 3 years at Tech Company\n\nSkills: Python, React",
        industry="tech_consulting",
        analysis_id="test-123",
        user_id="user-456",
        current_stage="preprocessing",
        has_errors=False,
        error_messages=[],
        retry_count=0,
        structure_errors=[],
        appeal_errors=[]
    )

class TestResumeAnalysisOrchestrator:
    """Test the main orchestrator functionality"""
    
    def test_preprocess_resume_success(self, orchestrator, sample_state):
        """Test successful resume preprocessing"""
        result = orchestrator.preprocess_resume(sample_state)
        
        assert result["has_errors"] is False
        assert result["current_stage"] == "preprocessing"
        assert "started_at" in result
        assert result["max_retries"] == 2
    
    def test_preprocess_resume_too_short(self, orchestrator):
        """Test preprocessing with too short resume"""
        short_state = AnalysisState(
            resume_text="Too short",
            industry="tech_consulting",
            analysis_id="test-short",
            user_id="user-456",
            current_stage="preprocessing",
            has_errors=False,
            error_messages=[],
            retry_count=0,
            structure_errors=[],
            appeal_errors=[]
        )
        
        result = orchestrator.preprocess_resume(short_state)
        
        assert result["has_errors"] is True
        assert "too short" in result["error_messages"][0].lower()
    
    def test_preprocess_invalid_industry(self, orchestrator):
        """Test preprocessing with invalid industry"""
        invalid_industry_state = AnalysisState(
            resume_text="A reasonably long resume text that should pass the length validation checks",
            industry="invalid_industry",
            analysis_id="test-invalid",
            user_id="user-456",
            current_stage="preprocessing",
            has_errors=False,
            error_messages=[],
            retry_count=0,
            structure_errors=[],
            appeal_errors=[]
        )
        
        result = orchestrator.preprocess_resume(invalid_industry_state)
        
        assert result["has_errors"] is True
        assert "unsupported industry" in result["error_messages"][0].lower()
    
    async def test_run_structure_agent_success(self, orchestrator, sample_state):
        """Test successful structure agent execution"""
        # Mock the structure agent
        orchestrator.structure_agent.analyze = AsyncMock(return_value={
            "structure_analysis": "mock_result",
            "structure_confidence": 0.85,
            "current_stage": "structure_analysis",
            "structure_errors": []
        })
        
        result = await orchestrator.run_structure_agent(sample_state)
        
        assert result["structure_confidence"] == 0.85
        assert result["current_stage"] == "structure_analysis"
        assert len(result["structure_errors"]) == 0
    
    async def test_run_structure_agent_failure(self, orchestrator, sample_state):
        """Test structure agent failure handling"""
        # Mock agent to raise exception
        orchestrator.structure_agent.analyze = AsyncMock(side_effect=Exception("Agent failed"))
        
        result = await orchestrator.run_structure_agent(sample_state)
        
        assert result["has_errors"] is True
        assert "structure analysis failed" in result["structure_errors"][0].lower()
    
    def test_validate_structure_results_success(self, orchestrator, sample_state):
        """Test successful structure validation"""
        # Set up state with good structure results
        sample_state["structure_analysis"] = MagicMock()
        sample_state["structure_analysis"].format_score = 85
        sample_state["structure_analysis"].section_organization_score = 80
        sample_state["structure_analysis"].professional_tone_score = 90
        sample_state["structure_analysis"].completeness_score = 75
        sample_state["structure_confidence"] = 0.85
        
        result = orchestrator.validate_structure_results(sample_state)
        
        assert result["current_stage"] == "structure_validated"
        assert len(result["structure_errors"]) == 0
    
    def test_validate_structure_low_confidence_retry(self, orchestrator, sample_state):
        """Test structure validation with low confidence triggering retry"""
        sample_state["structure_analysis"] = MagicMock()
        sample_state["structure_confidence"] = 0.3  # Below threshold
        sample_state["retry_count"] = 0
        sample_state["max_retries"] = 2
        
        result = orchestrator.validate_structure_results(sample_state)
        
        assert "low confidence" in result["structure_errors"][0].lower()
        assert result["retry_count"] == 1
    
    def test_validate_structure_low_confidence_max_retries(self, orchestrator, sample_state):
        """Test structure validation failing after max retries"""
        sample_state["structure_analysis"] = MagicMock()
        sample_state["structure_confidence"] = 0.3
        sample_state["retry_count"] = 2  # At max retries
        sample_state["max_retries"] = 2
        
        result = orchestrator.validate_structure_results(sample_state)
        
        assert result["has_errors"] is True
        assert "too low after" in result["error_messages"][0].lower()
    
    def test_routing_logic(self, orchestrator, sample_state):
        """Test conditional routing logic"""
        # Test successful routing
        route = orchestrator.route_after_preprocess(sample_state)
        assert route == "continue"
        
        # Test error routing
        error_state = sample_state.copy()
        error_state["has_errors"] = True
        route = orchestrator.route_after_preprocess(error_state)
        assert route == "error"
        
        # Test retry routing
        retry_state = sample_state.copy()
        retry_state["structure_errors"] = ["Some error"]
        retry_state["retry_count"] = 0
        retry_state["max_retries"] = 2
        route = orchestrator.route_after_structure(retry_state)
        assert route == "retry"
```

### **Integration Testing**

```python
# /tests/integration/test_full_workflow.py
import pytest
from app.ai.orchestrator import ResumeAnalysisOrchestrator
from app.ai.integrations.openai_client import OpenAIClient
from app.ai.models.analysis_request import AnalysisState

@pytest.mark.integration
class TestFullWorkflow:
    """Integration tests for complete workflow"""
    
    @pytest.fixture
    def real_orchestrator(self):
        """Create orchestrator with real LLM client"""
        # Use test API key or mock in CI
        llm_client = OpenAIClient(api_key="test-key", model="gpt-3.5-turbo")  
        return ResumeAnalysisOrchestrator(llm_client)
    
    @pytest.fixture  
    def comprehensive_resume_text(self):
        """Comprehensive resume for testing"""
        return """
        John Smith
        Senior Software Engineer
        john.smith@email.com | (555) 123-4567
        
        SUMMARY
        Experienced software engineer with 5+ years in full-stack development.
        
        EXPERIENCE
        Senior Software Engineer | TechCorp Inc. | 2020-Present
        - Led team of 8 developers on React/Node.js applications
        - Improved system performance by 40% through optimization
        - Architected microservices handling 1M+ daily requests
        
        Software Engineer | StartupXYZ | 2018-2020  
        - Developed REST APIs using Python/Django
        - Collaborated with cross-functional teams on product features
        
        EDUCATION
        B.S. Computer Science | University of Technology | 2018
        
        SKILLS
        Languages: Python, JavaScript, TypeScript, SQL
        Frameworks: React, Node.js, Django, FastAPI
        Tools: Docker, AWS, Git, PostgreSQL
        
        PROJECTS
        - E-commerce Platform: Built scalable platform serving 10k+ users
        - Data Analytics Dashboard: Real-time analytics using React/D3
        """
    
    async def test_complete_workflow_success(self, real_orchestrator, comprehensive_resume_text):
        """Test complete workflow from start to finish"""
        initial_state = AnalysisState(
            resume_text=comprehensive_resume_text,
            industry="tech_consulting", 
            analysis_id="integration-test-1",
            user_id="test-user",
            current_stage="preprocessing",
            has_errors=False,
            error_messages=[],
            structure_errors=[],
            appeal_errors=[],
            retry_count=0
        )
        
        # Execute complete workflow
        result = await real_orchestrator.app.ainvoke(initial_state)
        
        # Verify successful completion
        assert result["current_stage"] == "complete"
        assert result["has_errors"] is False
        assert result["final_result"] is not None
        assert result["overall_score"] > 0
        
        final_result = result["final_result"]
        
        # Verify structure analysis
        assert final_result.structure_analysis is not None
        structure = final_result.structure_analysis
        assert 0 <= structure.format_score <= 100
        assert 0 <= structure.section_organization_score <= 100
        assert 0 <= structure.professional_tone_score <= 100
        assert 0 <= structure.completeness_score <= 100
        assert len(structure.strengths) > 0
        assert len(structure.recommendations) >= 0
        
        # Verify appeal analysis
        assert final_result.appeal_analysis is not None
        appeal = final_result.appeal_analysis
        assert 0 <= appeal.achievement_relevance_score <= 100
        assert 0 <= appeal.skills_alignment_score <= 100
        assert 0 <= appeal.experience_fit_score <= 100
        assert 0 <= appeal.competitive_positioning_score <= 100
        assert appeal.market_tier in ["entry", "mid", "senior", "executive"]
        
        # Verify overall result
        assert final_result.industry == "tech_consulting"
        assert final_result.analysis_id == "integration-test-1"
        assert final_result.overall_score > 0
        assert len(final_result.analysis_summary) > 0
        
    async def test_workflow_with_different_industries(self, real_orchestrator, comprehensive_resume_text):
        """Test workflow with different industries produces different results"""
        industries = ["tech_consulting", "finance_banking", "healthcare_pharma"]
        results = {}
        
        for industry in industries:
            initial_state = AnalysisState(
                resume_text=comprehensive_resume_text,
                industry=industry,
                analysis_id=f"test-{industry}",
                user_id="test-user",
                current_stage="preprocessing",
                has_errors=False,
                error_messages=[],
                structure_errors=[],
                appeal_errors=[],
                retry_count=0
            )
            
            result = await real_orchestrator.app.ainvoke(initial_state)
            results[industry] = result["final_result"]
        
        # Verify that different industries produce different appeal analyses
        tech_appeal = results["tech_consulting"].appeal_analysis
        finance_appeal = results["finance_banking"].appeal_analysis
        healthcare_appeal = results["healthcare_pharma"].appeal_analysis
        
        # Appeal scores should be different (tech should be higher for this resume)
        assert tech_appeal.skills_alignment_score != finance_appeal.skills_alignment_score
        assert tech_appeal.experience_fit_score != healthcare_appeal.experience_fit_score
        
        # Industry-specific feedback should be different
        tech_keywords = set(tech_appeal.industry_keywords)
        finance_keywords = set(finance_appeal.industry_keywords) 
        assert tech_keywords != finance_keywords
    
    async def test_error_recovery_workflow(self, real_orchestrator):
        """Test workflow error recovery mechanisms"""
        # Test with problematic input that might cause parsing errors
        problematic_resume = "Short resume with minimal content that might cause issues."
        
        initial_state = AnalysisState(
            resume_text=problematic_resume,
            industry="tech_consulting",
            analysis_id="error-recovery-test",
            user_id="test-user", 
            current_stage="preprocessing",
            has_errors=False,
            error_messages=[],
            structure_errors=[],
            appeal_errors=[],
            retry_count=0
        )
        
        result = await real_orchestrator.app.ainvoke(initial_state)
        
        # Workflow should either complete successfully or fail gracefully
        if result["current_stage"] == "complete":
            # Successful completion
            assert result["final_result"] is not None
            assert result["overall_score"] > 0
        else:
            # Graceful failure
            assert result["current_stage"] == "error"
            assert result["has_errors"] is True
            assert len(result["error_messages"]) > 0
```

## ðŸš€ Deployment & Configuration

### **Environment Configuration**

Add these settings to your `app/core/config.py`:

```python
class Settings(BaseSettings):
    # ... existing settings ...
    
    # LangGraph Configuration
    LANGGRAPH_WORKFLOW_TIMEOUT_SECONDS: int = 300
    LANGGRAPH_MAX_RETRIES: int = 2
    LANGGRAPH_ENABLE_CHECKPOINTS: bool = True
    
    # Agent Configuration  
    STRUCTURE_AGENT_CONFIDENCE_THRESHOLD: float = 0.6
    APPEAL_AGENT_CONFIDENCE_THRESHOLD: float = 0.65
    
    # OpenAI Configuration
    OPENAI_API_KEY: str
    OPENAI_MODEL_NAME: str = "gpt-4"
    OPENAI_MAX_TOKENS: int = 4000
    OPENAI_TEMPERATURE: float = 0.3
    OPENAI_REQUEST_TIMEOUT_SECONDS: int = 30
    
    # Monitoring
    ENABLE_AI_WORKFLOW_LOGGING: bool = True
    AI_METRICS_COLLECTION_ENABLED: bool = True
    
    class Config:
        env_file = ".env"
```

### **Dependencies Installation**

Update your `requirements.txt`:

```txt
# LangGraph and LangChain
langgraph>=0.2.0
langchain>=0.2.0
langchain-openai>=0.1.0

# Pydantic for structured data
pydantic>=2.0.0

# Token counting and text processing
tiktoken>=0.7.0

# Async HTTP client for OpenAI
httpx>=0.25.0
aiohttp>=3.8.0

# For development and testing
pytest-asyncio>=0.23.0
pytest-mock>=3.10.0
```

## ðŸ“ˆ Performance Monitoring

### **Workflow Performance Tracking**

```python
# /app/ai/monitoring.py
import time
import logging
from typing import Dict, Any
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class WorkflowMonitor:
    """Monitor LangGraph workflow performance and health"""
    
    def __init__(self):
        self.stage_timers = {}
        self.workflow_metrics = {}
    
    @asynccontextmanager
    async def track_stage(self, analysis_id: str, stage: str):
        """Context manager to track workflow stage performance"""
        start_time = time.time()
        logger.info(f"Starting stage '{stage}' for analysis {analysis_id}")
        
        try:
            yield
        finally:
            duration = time.time() - start_time
            logger.info(f"Completed stage '{stage}' for analysis {analysis_id} in {duration:.2f}s")
            
            # Store metrics
            if analysis_id not in self.workflow_metrics:
                self.workflow_metrics[analysis_id] = {}
            self.workflow_metrics[analysis_id][stage] = {
                "duration_seconds": duration,
                "timestamp": time.time()
            }
    
    def log_workflow_completion(self, analysis_id: str, final_state: Dict[str, Any]):
        """Log complete workflow metrics"""
        if analysis_id in self.workflow_metrics:
            stages = self.workflow_metrics[analysis_id]
            total_time = sum(stage["duration_seconds"] for stage in stages.values())
            
            logger.info(
                f"Workflow completed for {analysis_id}",
                extra={
                    "analysis_id": analysis_id,
                    "total_duration_seconds": total_time,
                    "stage_durations": {k: v["duration_seconds"] for k, v in stages.items()},
                    "final_stage": final_state.get("current_stage"),
                    "success": not final_state.get("has_errors", False),
                    "overall_score": final_state.get("overall_score"),
                    "retry_count": final_state.get("retry_count", 0)
                }
            )
            
            # Clean up metrics to prevent memory leak
            del self.workflow_metrics[analysis_id]

# Usage in orchestrator
monitor = WorkflowMonitor()

async def run_structure_agent(self, state: AnalysisState) -> Dict[str, Any]:
    async with monitor.track_stage(state["analysis_id"], "structure_analysis"):
        return await self.structure_agent.analyze(state)
```

## ðŸŽ¯ Common Pitfalls and Solutions

### **1. State Mutation Issues**
```python
# âŒ WRONG: Mutating input state
def bad_node(state: AnalysisState) -> Dict[str, Any]:
    state["current_stage"] = "processing"  # Don't modify input state
    return state  # Don't return entire state

# âœ… CORRECT: Return only updates
def good_node(state: AnalysisState) -> Dict[str, Any]:
    # Process without mutating input
    result = process_data(state["input"])
    
    # Return only the updates needed
    return {
        "current_stage": "processing",
        "results": result
    }
```

### **2. Missing Error Handling in Nodes**
```python
# âŒ WRONG: No error handling
def bad_node(state: AnalysisState) -> Dict[str, Any]:
    result = risky_operation(state["data"])  # Can crash workflow
    return {"result": result}

# âœ… CORRECT: Comprehensive error handling
def good_node(state: AnalysisState) -> Dict[str, Any]:
    try:
        result = risky_operation(state["data"])
        return {
            "result": result,
            "current_stage": "completed",
            "errors": []
        }
    except Exception as e:
        logger.error(f"Node failed: {str(e)}", exc_info=True)
        return {
            "has_errors": True,
            "error_messages": [f"Node failed: {str(e)}"],
            "current_stage": "error"
        }
```

### **3. Inefficient Conditional Routing**
```python
# âŒ WRONG: Complex routing logic in routing function
def bad_routing(state: AnalysisState) -> str:
    # Too much logic here - hard to test and debug
    if state.get("confidence", 0) < 0.6:
        if state.get("retry_count", 0) < 3:
            if time.time() - state.get("last_retry", 0) > 60:
                return "retry"
        return "error"
    return "continue"

# âœ… CORRECT: Simple routing with logic in validation nodes
def good_routing(state: AnalysisState) -> str:
    if state.get("has_errors"):
        return "error"
    if state.get("needs_retry"):
        return "retry"
    return "continue"

def validation_node(state: AnalysisState) -> Dict[str, Any]:
    # Complex logic handled in dedicated validation node
    confidence = state.get("confidence", 0)
    retry_count = state.get("retry_count", 0)
    
    if confidence < 0.6 and retry_count < 3:
        return {
            "needs_retry": True,
            "retry_count": retry_count + 1
        }
    elif confidence < 0.6:
        return {
            "has_errors": True,
            "error_messages": ["Confidence too low after max retries"]
        }
    
    return {"needs_retry": False}
```

## ðŸŽ“ Next Steps

### **After Implementing the Basic Workflow**

1. **Advanced Features**:
   - Implement parallel agent execution
   - Add human-in-the-loop capabilities
   - Create custom LangGraph checkpoints

2. **Performance Optimization**:
   - Implement caching for repeated analyses
   - Add batch processing capabilities
   - Optimize LLM token usage

3. **Production Readiness**:
   - Add comprehensive monitoring
   - Implement proper error alerting
   - Create deployment automation

### **Additional Learning Resources**

- **LangGraph Examples**: https://github.com/langchain-ai/langgraph/tree/main/examples
- **Multi-Agent Tutorials**: https://langchain-ai.github.io/langgraph/tutorials/multi_agent/
- **Production Patterns**: https://langchain-ai.github.io/langgraph/how-tos/

---

**Document Status**: Complete Implementation Guide  
**Last Updated**: September 4, 2025  
**Next Review**: After Sprint 004 completion

*This guide provides everything needed to implement the LangGraph AI orchestration system successfully. Follow the patterns, test thoroughly, and monitor performance for production-ready AI workflows.*